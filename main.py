# fichier : main.py
from fastapi import FastAPI, Query, HTTPException, Body, Form, status
from pydantic import BaseModel
import whisper
import requests
from fastapi.middleware.cors import CORSMiddleware
import os
from typing import Union
import uuid
from fastapi import UploadFile, Form
from fastapi.responses import JSONResponse
from utils.diarization import extract_speakers, write_segments
from fastapi.responses import JSONResponse
from dotenv import load_dotenv
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.prompts import ChatPromptTemplate
from typing import List, Dict
import json
import torchaudio
import math
import numpy as np
import re
import torch
from speechbrain.inference.speaker import EncoderClassifier
from pyannote.audio.pipelines.speaker_verification import PretrainedSpeakerEmbedding
from pyannote.audio import Pipeline
from scipy.spatial.distance import cdist
import sqlite3
from utils.utils import extract_speaker_embeddings, find_nearest_speaker, format_time
from langchain_community.tools import WikipediaQueryRun
from langchain_community.utilities import WikipediaAPIWrapper
from utils.prompt import generate_fact_check_prompt, generate_fact_extraction_prompt
from utils.splitter import clean_json_string
from langchain_tavily import TavilySearch

# -----------------------------
# CONFIG
# -----------------------------
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
DB_FILE = "speakers_store/spearker_voice.db"


load_dotenv('.env.local')
HF_TOKEN = os.getenv("HF_TOKEN")
os.environ["SERPAPI_API_KEY"] = "17b4254cd1caa589a23ab4e13821a0784d17aa09450c3b94f1249ef3dd5313ad"
SERPER_API_KEY = "5c51947ee6b3e5fcf45e7ff19e4652f748168e70"
os.environ["SERPER_API_KEY"] = "5c51947ee6b3e5fcf45e7ff19e4652f748168e70"
os.environ["TAVILY_API_KEY"] = "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"
TAVILY_API_KEY = "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"
model_gemini = ChatGoogleGenerativeAI(model="gemini-2.0-flash-lite", google_api_key="AIzaSyAv5QViz6g3qDDiaX-jI3bzZ2HEf-7rXL0")
classifier = EncoderClassifier.from_hparams(
    source="speechbrain/spkrec-ecapa-voxceleb",
    run_opts={"device": DEVICE}
).to(DEVICE)
pipeline = Pipeline.from_pretrained(
    "pyannote/speaker-diarization-3.1",
    use_auth_token=HF_TOKEN
)

llm = whisper.load_model("turbo")  # ou "small", "medium", "large"
app = FastAPI()
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"], 
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

@app.get("/")
def root():
    return {"status": "API is running"}

@app.get("/health")
def health():
    return {"status": "healthy"}

@app.post("/media")
async def create_media(
    name: str = Form(...),
    description: str = Form(None),
    numberSpeaker: int = Form(1),
    audio: UploadFile = None,
    video: UploadFile = None
):
    os.makedirs("uploads", exist_ok=True)

    audio_link, video_link, file_size = None, None, 0

    # Sauvegarde de l'audio
    if audio:
        audio_filename = f"{uuid.uuid4()}_{audio.filename}"
        audio_path = os.path.join("uploads", audio_filename)
        with open(audio_path, "wb") as f:
            f.write(await audio.read())
        audio_link = f"/uploads/{audio_filename}"
        file_size = audio.size or 0

    # Sauvegarde de la vid√©o
    if video:
        video_filename = f"{uuid.uuid4()}_{video.filename}"
        video_path = os.path.join("uploads", video_filename)
        with open(video_path, "wb") as f:
            f.write(await video.read())
        video_link = f"/uploads/{video_filename}"
        file_size = video.size or 0

    # Exemple de retour JSON (tu peux l‚Äôadapter pour ins√©rer en base avec Prisma si tu veux)
    return JSONResponse(
        {
            "name": name,
            "description": description,
            "numberSpeaker": numberSpeaker,
            "audioLink": audio_link,
            "videoLink": video_link,
            "fileSize": file_size,
            "message": "Media uploaded successfully üöÄ"
        }
    )

    
class TranscriptionResponse(BaseModel):
    text: str

class DiarisationResponse(BaseModel):
    transcript: str

# ====== Mod√®les Pydantic =======

class SpeakerSegment(BaseModel):
    speaker: str
    start_time: str
    text: str

class Transcript(BaseModel):
    transcript: List[SpeakerSegment]

class TranscriptItem(BaseModel):
    speaker: str
    start_time: str
    text: str

class Transcript(BaseModel):
    transcript: List[TranscriptItem]

class FactCheckItem(BaseModel):
    statement: str
    verdict: str
    sources: List[str]
    confidence: float
    category: str
    start_time: str
    justification: str
    evidence: str
    speaker: str

class FactCheckRequest(BaseModel):
    transcript: List[TranscriptItem]

class IdeologyResult(BaseModel):
    speaker: str
    orientation: str
    justification: str
    confidence: float

class PsychProfile(BaseModel):
    speaker: str
    dominant_traits: List[str]
    tone: str
    intentions: List[str]
    confidence: float

class SummaryResult(BaseModel):
    global_summary: str
    by_speaker: Dict[str, str]
    
# AI agent tools
def serper_video_search(query: str) -> str:
    url = "https://google.serper.dev/videos"
    headers = {
        "X-API-KEY": SERPER_API_KEY,
        "Content-Type": "application/json"
    }
    body = {"q": query}

    response = requests.post(url, headers=headers, data=json.dumps(body))
    data = response.json()

    if "videos" in data:
        results = []
        for vid in data["videos"]:
            title = vid.get("title", "")
            link = vid.get("link", "")
            snippet = vid.get("snippet", "")
            results.append(f"- Titre: {title}\n  Lien: {link}\n  R√©sum√©: {snippet}")
        return "\n".join(results)
    else:
        return "Aucune vid√©o pertinente trouv√©e."
    
# Initialize Tavily Search Tool
tavily_search_tool = TavilySearch(
    max_results=5,
    topic="general",
    include_answer=True
)

wikipedia_tool = WikipediaQueryRun(api_wrapper=WikipediaAPIWrapper())

def parse_single_psych_profile(output_text: str, speaker: str) -> PsychProfile:
    # Extraire Traits
    traits_match = re.search(r"Traits\s*:\s*(.*?)\n\s*Ton\s*:", output_text, re.DOTALL | re.IGNORECASE)
    dominant_traits = []
    if traits_match:
        traits_block = traits_match.group(1)
        dominant_traits = [line.strip(" *") for line in traits_block.strip().splitlines() if line.strip()]

    # Extraire Ton
    tone_match = re.search(r"Ton\s*:\s*\*\s*(.*?)\n\s*Intentions\s*:", output_text, re.DOTALL | re.IGNORECASE)
    tone = tone_match.group(1).strip() if tone_match else ""

    # Extraire Intentions
    intentions_match = re.search(r"Intentions\s*:\s*(.*?)\n\s*Confiance\s*:", output_text, re.DOTALL | re.IGNORECASE)
    intentions = []
    if intentions_match:
        intentions_block = intentions_match.group(1)
        intentions = [line.strip(" *") for line in intentions_block.strip().splitlines() if line.strip()]

    # Extraire Confiance
    confidence_match = re.search(r"Confiance\s*:\s*\*\s*(\d+)%", output_text, re.IGNORECASE)
    confidence = float(confidence_match.group(1)) if confidence_match else 0.0

    return PsychProfile(
        speaker=speaker,
        dominant_traits=dominant_traits,
        tone=tone,
        intentions=intentions,
        confidence=confidence
    )

@app.post("/transcription/internal")
async def transcription(audio_id: str = Query(..., description="identifiant de l'audio h√©berg√©e")):
    try:
        audio_data = os.path.join("uploads", audio_id)
        if not os.path.exists(audio_data):
            raise HTTPException(status_code=404, detail="File not found")
        else:
            print("audio found")
            # Transcription locale avec Whisper
            result = llm.transcribe(audio_data)
            return {"text": result["text"]}

    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/diarisation/internal")
async def diarisation(audio_id: str = Query(..., description="identifiant de l'audio h√©berg√©e"), num_speakers: int = Query(..., description="Nombre de locuteurs")):
    try:
        audio_data = os.path.join("uploads", audio_id)
        if not os.path.exists(audio_data):
            raise HTTPException(status_code=404, detail="File not found")
        else:
            print("audio found")

            # Diarisation locale avec Whisper
            seg = extract_speakers(llm, classifier, audio_data,num_speakers)
            result = write_segments(seg, 'temp/transcript.txt')
            return JSONResponse(content=result)
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/uploads/")
async def create_upload_file(file: Union[UploadFile, None] = None):
    try:
        if not file:
            raise HTTPException(status_code=400, detail="No upload file sent")
        else:
            filename=str(uuid.uuid4())+"_"+file.filename
            file_path=os.path.join("uploads", filename)
            print(file_path)
            with open(file_path, "wb") as f:
                f.write(file.file.read())
            return {"message": "File saved successfully","filename":filename}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/file/{name_file}")
def get_file(name_file: str):
    try:
        if not os.path.exists(os.path.join("uploads", name_file)):
            raise HTTPException(status_code=404, detail="File not found")
        else:
            return FileResponse(path=os.path.join("uploads", name_file))
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/fact-checking")
async def fact_check_agent(transcript: Transcript):
    results = []
    for seg in transcript.transcript:
        prompt = generate_fact_extraction_prompt(seg.text, seg.speaker, seg.start_time)
        result = model_gemini.invoke(prompt)
        parse_result = clean_json_string(result.content)
        parse_result: List[Dict] = json.loads(parse_result)
        tavily_search_result = ""
        wikipedia_search_result = ""
        serper_video_search_result = ""
        for fact in parse_result:
            
            for query in fact["query_tavily"]:
                tems = tavily_search_tool.invoke({'query': query})
                tavily_search_result += json.dumps(tems)
            
            for query in fact["query_wikipedia"]:
                wikipedia_search_result += json.dumps(wikipedia_tool.run(query))
            
            for query in fact["query_serper_video"]:
                serper_video_search_result += serper_video_search(query)
            
        fact_check_prompt = generate_fact_check_prompt(fact["statement"], seg.speaker, seg.start_time, tavily_search_result, wikipedia_search_result, serper_video_search_result)
        print(fact_check_prompt)
        result = model_gemini.invoke(fact_check_prompt)
        print(result.content)
        parse_result = clean_json_string(result.content)
        parse_result = json.loads(parse_result)
        results.append(parse_result)

    return results


@app.post("/ideology", response_model=List[IdeologyResult])
async def ideology(transcript: Transcript):
    # Concat√®ne le texte par intervenant
    speakers_text = {}
    for seg in transcript.transcript:
        speakers_text.setdefault(seg.speaker, "")
        speakers_text[seg.speaker] += seg.text + " "

    # Construit le bloc √† analyser
    speakers_formatted = "\n\n".join(
        [f"{speaker}:\n{speakers_text[speaker]}" for speaker in speakers_text]
    )

    # Prompt journalistique expert pour l'id√©ologie
    prompt = ChatPromptTemplate.from_template("""
    Vous √™tes un analyste politique expert.
    Votre mission est d'examiner attentivement les propos de chaque intervenant afin d'identifier son orientation id√©ologique ou politique dominante.

    **Consignes :**
    1Ô∏è‚É£ Pour chaque intervenant, indiquez son nom ou identifiant.
    2Ô∏è‚É£ D√©crivez son **orientation id√©ologique** dominante (ex. : progressiste, conservateur, centriste, radical, lib√©ral, socialiste‚Ä¶).
    3Ô∏è‚É£ Justifiez bri√®vement votre analyse √† partir des propos tenus, sans extrapoler au-del√† du contenu fourni.
    4Ô∏è‚É£ Attribuez un **niveau de confiance** en pourcentage (0-100) pour refl√©ter la solidit√© de votre estimation.
    5Ô∏è‚É£ R√©digez en fran√ßais formel, neutre, sans jugement de valeur.
    6Ô∏è‚É£ Chaque element de la r√©ponse doit etre sur une ligne
    7Ô∏è‚É£ Sois strict, professionnel, pr√©cis et rigoureux et sans texte hors structure.
    

    **Format attendu :**
    Intervenant : [nom]
    Orientation : [orientation id√©ologique]
    Justification : [justification]
    Confiance : [pourcentage]

    **Exemples :**

    Exemple 1 :
    Intervenant : Mme Dupont
    Orientation : Progressiste
    Justification : Mme Dupont d√©fend des mesures sociales inclusives, √©voque la redistribution des richesses et insiste sur la solidarit√© nationale.
    Confiance : 85

    Exemple 2 :
    Intervenant : M. Martin
    Orientation : Conservateur
    Justification : M. Martin met l'accent sur la tradition, critique les r√©formes soci√©tales rapides et valorise la stabilit√© familiale.
    Confiance : 90


    **Texte √† analyser :**
    {synthese}
    """)

    chain = prompt | model_gemini

    result = chain.invoke({"synthese": speakers_formatted})

    # Traitement minimal du texte brut (prototype)
    lines = result.content.split("\n")
    results = []
    current = {}

    for line in lines:
        if line.strip().startswith("Intervenant"):
            if current:
                results.append(current)
            current = {}
            current['speaker'] = line.split(":")[1].strip()
        elif "Orientation" in line:
            current['orientation'] = line.split(":")[1].strip()
        elif "Justification" in line:
            current['justification'] = line.split(":")[1].strip()
        elif "Confiance" in line:
            conf = line.split(":")[1].strip().replace("%", "")
            current['confidence'] = float(conf)

    if current:
        results.append(current)

    return [IdeologyResult(**r) for r in results]

@app.post("/psych-profile", response_model=List[PsychProfile])
async def analyze_psych_profile(transcript: Transcript):

    # üß† 2Ô∏è‚É£ Profilage psychologique
    speakers = {}
    for seg in transcript.transcript:
        speakers.setdefault(seg.speaker, []).append(seg.text)

    results = []
    for speaker, texts in speakers.items():
        full_text = " ".join(texts)

        prompt = f"""
        üîç Vous √™tes un psychologue et sociologue expert depuis plus de 50 ans, sp√©cialis√© en analyse conversationnelle.  
        Votre mission est d‚Äô√©tablir un **profil psychologique pr√©cis** pour chaque intervenant, bas√© sur **leur discours** ET les r√©sultats du **fact-checking**.
        ---

        **üëâ EXEMPLES DE FORMAT √Ä RESPECTER ABSOLUMENT :**

        EXEMPLE 1
        Traits :
            * Curieux (pose plusieurs questions ouvertes)
            * Pr√©cis (relance sur des d√©tails chiffr√©s)
            * Observateur (note les nuances dans le discours)
        Ton :
            * Neutre (questions factuelles sans jugement)
        Intentions :
            * Comprendre (cherche des pr√©cisions)
            * V√©rifier (confirme des affirmations)
        Confiance :
            * 80%
        ---

        EXEMPLE 2
        Traits :
            * Ambitieux (√©voque des objectifs √©lev√©s)
            * Strat√®ge (met en avant des plans √† long terme)
        Ton :
            * Affirmatif (discours assur√©)
        Intentions :
            * Convaincre (veut rallier √† sa vision)
            * Inspirer (donne envie de suivre son plan)
        Confiance :
            * 95%

        ---

        EXEMPLE 3
        Traits :
            * Sceptique (pose des questions critiques)
            * Analytique (souligne les contradictions)
        Ton :
            * Direct (discours sans d√©tour)
        Intentions :
            * D√©stabiliser (met en difficult√©)
            * Tester (v√©rifie la solidit√© du discours)
        Confiance :
            * 75%

        ---

        **üëâ TEXTE √Ä ANALYSER :**

        \"\"\"{full_text}\"\"\"

        ---

        **üéØ T√¢che :**
        - Identifiez 2-5 traits psychologiques cl√©s (chaque trait doit √™tre justifi√©).
        - Identifiez le ton principal (justification).
        - Identifiez 3-5 intentions dominantes (justification).
        - Donnez un pourcentage de confiance global (justification implicite).

        ---

        **üìå Rappel importants:**  
        - Recherchers les informations sur la veracites des affirmations sur internet.
        - Respectez **scrupuleusement le format EXACT des EXEMPLES.**  
        - Ne changez jamais l'ordre : Traits ‚ûú Ton ‚ûú Intentions ‚ûú Confiance.
        - Une seule analyse par intervenant.
        - Ne mettez aucun texte hors structure.
        - Sois pr√©cis, concis et rigoureux.

        """
        result = model_gemini.invoke(prompt)

        profile = parse_single_psych_profile(result.content,speaker)
        results.append(profile)

    return results


@app.post("/summarize", response_model=SummaryResult)
def summarize(transcript: Transcript):
    all_text = "\n\n".join(
        [f"[{seg.speaker}] {seg.text}" for seg in transcript.transcript]
    )
    # 2Ô∏è‚É£ Pr√©pare le prompt LangChain
    prompt = ChatPromptTemplate.from_template("""
    Vous √™tes un r√©dacteur et analyste professionnel sp√©cialis√© dans le journalisme de synth√®se.  
    Votre mission est de produire une synth√®se claire, objective et hi√©rarchis√©e d‚Äôun dialogue, d√©bat ou interview, tout en respectant la parole de chaque intervenant.  
    Votre style doit √™tre neutre, √©l√©gant, et factuel, comme dans un compte-rendu destin√© √† la presse ou √† une revue s√©rieuse.

    Consignes :
    Le resume consiste a dire ce qui est dit dans le dialogue en le reformulant de facon synth√©tique et concise.
    1Ô∏è‚É£ R√©digez d'abord un r√©sum√© global detaille du contenu, en pr√©sentant le contexte, le ton g√©n√©ral et les principaux points abord√©s, sans jugement et dire ce qui est dit par rapport a ces points.
    2Ô∏è‚É£ Pour chaque intervenant, r√©digez ensuite un **r√©sum√© individuel**, en indiquant son nom ou son identifiant et en reformulant ce qu'il dit.  
    3Ô∏è‚É£ Lorsque vous parlez de chaque intervenant, vouvoyez-le syst√©matiquement, en rappelant ses propos avec precision et de facon detaille en utilisant le vous.  
    4Ô∏è‚É£ Votre style doit √™tre fluide, structur√©, r√©dig√© en fran√ßais formel, sans familiarit√© excessive.
    5Ô∏è‚É£ N'inventez pas de contenu : reformulez uniquement ce qui figure explicitement dans le dialogue fourni.
    6Ô∏è‚É£ Le resume doit etre humain et factuel et doit etre structur√©.
    8Ô∏è‚É£ Tu dois egalement respecte le format de reponse exactement et repondre chaque ca sur une ligne (GLOBAL, SPEAKER 1,SPEAKER 2,...) comme ce dernier.


    Format :
    GLOBAL: ...
    SPEAKER:Nom : R√©sum√©

    Exemple :
    GLOBAL: la dialogue parle en general de ...
    Thimote: Il soutinent le point selon lequel ...
    David: Selon David ...


    Dialogue √† analyser :
    {dialogue}

    """)



    # 3Ô∏è‚É£ G√©n√®re
    print(prompt)
    chain = prompt | model_gemini

    result = chain.invoke({"dialogue": all_text})
    print(result.content)

    # 4Ô∏è‚É£ Parse (simplifi√©)
    lines = result.content.split("\n")
    global_summary = None
    by_speaker = {}
    for line in lines:
        print(f"start {line} end")
        if line.startswith("GLOBAL"):
            global_summary = line.replace("GLOBAL:", "").strip()
        else:
            parts = line.strip().split(":")
            if len(parts) >= 2:
                speaker = parts[0].strip()
                summary = ":".join(parts[1:]).strip()
                by_speaker[speaker] = summary

    return SummaryResult(global_summary=global_summary, by_speaker=by_speaker)

@app.post("/add-speaker-voice", response_model=Dict[str, Union[str, int]])
async def add_speaker(speaker_name: str = Form(...), file: UploadFile = None):
    # Sauver temporairement le fichier audio
    tmp_path = f"/tmp/voice/{speaker_name}.wav"
    
    if not os.path.exists(tmp_path):
        os.makedirs(os.path.dirname(tmp_path), exist_ok=True)
    
    with open(tmp_path, "wb") as f:
        f.write(await file.read())
    
    # Extraire l'embedding
    embedding = extract_speaker_embeddings(tmp_path, DEVICE,classifier)
    embedding_json = json.dumps(embedding.tolist())

    with sqlite3.connect(DB_FILE) as conn:
        conn.execute(
            "INSERT INTO speakers (speaker_name, embedding) VALUES (?, ?)",
            (speaker_name, embedding_json)
        )

    return {"message": f"Speaker '{speaker_name}' ajout√© avec succ√®s.", "embedding_dim": len(embedding)}

@app.get("/all-speakers-voice")
async def list_speakers():
    with sqlite3.connect(DB_FILE) as conn:
        cur = conn.cursor()
        cur.execute("SELECT id, speaker_name FROM speakers")
        rows = cur.fetchall()
    
    return [{"id": row[0], "speaker_name": row[1]} for row in rows]

@app.post("/identify-speaker", response_model=Dict[str, Union[str, float]])
async def identify_speaker_sqlite(file: UploadFile):
    tmp_path = f"/tmp/{file.filename}.wav"
    if not os.path.exists(tmp_path):
        os.makedirs(os.path.dirname(tmp_path), exist_ok=True)
    
    with open(tmp_path, "wb") as f:
        f.write(await file.read())  
    
    new_emb = extract_speaker_embeddings(tmp_path, DEVICE,classifier)
    
    best_match = find_nearest_speaker(new_emb, DB_FILE)

    if best_match:
        return {"speaker": best_match["speaker"], "similarity": best_match["similarity"]}
    else:
        return {"speaker": None, "similarity": 0.0}

@app.post("/asr")
async def asr(file: UploadFile):

    tmp_path = f"/tmp/{file.filename}.wav"
    if not os.path.exists(tmp_path):
        os.makedirs(os.path.dirname(tmp_path), exist_ok=True)
    
    with open(tmp_path, "wb") as f:
        f.write(await file.read()) 
    
    transcript = llm.transcribe(tmp_path)
    segments = pipeline(tmp_path)
    
    # Set a threshold for similarity scores to determine when a match is considered successful
    threshold = 0.7
    sample_rate=44100
    speaker_segments = []
    aligned_output = []

    # Iterate through each segment identified in the diarization process
    for segment, label, confidence in segments.itertracks(yield_label=True):
        start_time, end_time = segment.start, segment.end

        # Load the specific audio segment from the meeting recording
        waveform, sample_rate = torchaudio.load(tmp_path, num_frames=int((end_time-start_time)*sample_rate), frame_offset=int(start_time*sample_rate))
        waveform = waveform.to(DEVICE)

        # Extract the speaker embedding from the audio segment
        embedding = classifier.encode_batch(waveform).squeeze(1).cpu().numpy()/np.linalg.norm(classifier.encode_batch(waveform).squeeze(1).cpu().numpy())

        # Initialize variables to find the recognized speaker
        min_distance = float('inf')
        recognized_speaker_id = None

        with sqlite3.connect(DB_FILE) as conn:
            cur = conn.cursor()
            cur.execute("SELECT id, speaker_name, embedding FROM speakers")
            rows = cur.fetchall()

        known_speaker_ids = [row[1] for row in rows]
        known_speakers = [np.array(json.loads(row[2])) for row in rows]

        # Compare the segment's embedding to each known speaker's embedding using cosine distance
        for i, speaker_embedding in enumerate(known_speakers):
            distances = cdist(embedding.tolist(), speaker_embedding, metric="cosine")
            min_distance_candidate = distances.min()
            if min_distance_candidate < min_distance:
                min_distance = min_distance_candidate
                recognized_speaker_id = known_speaker_ids[i]

        # Output the identified speaker and the time range they were speaking, if a match is found
        if min_distance < threshold and end_time-start_time>1:
            speaker_segments.append({
                "speaker": recognized_speaker_id,
                "start_time": math.floor(float(start_time)),
                "end_time": math.floor(float(end_time)),
            })
        elif end_time-start_time > 1:
            speaker_segments.append({
                "speaker": "unknown",
                "start_time": start_time,
                "end_time": end_time,
            })
    
    for segment in transcript['segments']   :
        start = segment['start']
        text = segment['text'].strip()
        start = math.floor(float(start))
        speak = None
        for seg in speaker_segments:
            if start >= seg["start_time"] and start < seg["end_time"]:
                speak = seg["speaker"]
                break
        if speak is None:
            speak = "unknown"
        timestamp = format_time(start)
        aligned_output.append({
            "speaker": speak,
            "timestamp": timestamp,
            "start": start,
            "end": segment['end'],
            "text": text
        })

    return aligned_output
